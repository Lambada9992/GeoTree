{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praca inżynierska\n",
    "Marcin Bobiński\n",
    "nr albumu: 297225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_PATH = os.getcwd()\n",
    "RESEARCH_PATH = os.path.join(HOME_PATH, \"models/research\")\n",
    "DATASET_PATH = os.path.join(HOME_PATH, \"Dataset\")\n",
    "PRETRAINED_MODELS_PATH = os.path.join(HOME_PATH, \"pretrained_models\")\n",
    "DATA_PATH = os.path.join(HOME_PATH, \"data\")\n",
    "TF_RECORD_PATH = os.path.join(DATA_PATH, \"tf_record\")\n",
    "MY_MODEL = os.path.join(HOME_PATH, \"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uczenie modelu\n",
    "## Przygotowanie plików tf record"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir(HOME_PATH)\n",
    "if not os.path.exists(MY_MODEL):\n",
    "    os.makedirs(MY_MODEL)\n",
    "\n",
    "_ = shutil.copy(\n",
    "    os.path.join(PRETRAINED_MODELS_PATH,\"ssd640\",\"pipeline.config\"),\n",
    "    MY_MODEL\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uczenie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training = f\"python object_detection/model_main_tf2.py \\\n",
    "--pipeline_config_path={os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "--model_dir={os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "--alsologtostderr\"\n",
    "print(training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval = f\"python object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "    --model_dir={os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "    --checkpoint_dir={os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "    --alsologtostderr\"\n",
    "print(eval)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensorboard = f\"tensorboard --logdir={os.path.join(MY_MODEL,'checkpoints')}\"\n",
    "print(tensorboard)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export the inference graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(RESEARCH_PATH)\n",
    "\n",
    "os.system(f\"python object_detection/exporter_main_v2.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path {os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "    --trained_checkpoint_dir {os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "    --output_directory {os.path.join(MY_MODEL, 'inference_graph')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export TF Lite graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(RESEARCH_PATH)\n",
    "\n",
    "os.system(f\"python object_detection/export_tflite_graph_tf2.py \\\n",
    "    --pipeline_config_path {os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "    --trained_checkpoint_dir {os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "    --output_directory {os.path.join(MY_MODEL, 'inference_tflite_graph')} \\\n",
    "    --max_detections 100 \\\n",
    "    --ssd_use_regular_nms False \\\n",
    "    \"\n",
    ")\n",
    "\n",
    "os.chdir(HOME_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(RESEARCH_PATH)\n",
    "\n",
    "os.system(f\"python object_detection/export_tflite_ssd_graph.py \\\n",
    "    --pipeline_config_path {os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "    --trained_checkpoint_prefix {os.path.join(MY_MODEL,'checkpoints','ckpt-21.index')} \\\n",
    "    --output_directory {os.path.join(MY_MODEL, 'inference_tflite_graph')} \\\n",
    "    --add_postprocessing_op True \\\n",
    "\")\n",
    "\n",
    "os.chdir(HOME_PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DEFAULT TFLITE\n",
    "import tensorflow as tf\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(MY_MODEL, 'inference_tflite_graph', 'saved_model'))\n",
    "\n",
    "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "os.chdir(HOME_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TFLITE QUANTIZATION INT8\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = os.path.join(DATASET_PATH, \"test\", \"Images\", \"05june05_static_street_boston\")\n",
    "\n",
    "def representative_dataset():\n",
    "    for image_path in os.listdir(dataset_path):\n",
    "        image = np.asarray(Image.open(os.path.join(dataset_path, image_path)).convert('RGB').resize((640,640)), dtype=np.float32)\n",
    "        image = np.array(np.expand_dims(image, 0), dtype=np.float32)\n",
    "        yield [image]\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "# Convert the model\\n\",\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(MY_MODEL, 'inference_tflite_graph', 'saved_model'))\n",
    "\n",
    "converter.allow_custom_ops = True\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\\n\",\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "os.chdir(HOME_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TFLITE FLOAT 16\n",
    "import tensorflow as tf\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(MY_MODEL, 'inference_tflite_graph', 'saved_model'))\n",
    "\n",
    "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "os.chdir(HOME_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mask-RCNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.saved_model.load(os.path.join(MY_MODEL,\"inference_graph\",\"saved_model\"))\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "keras_model = model.keras_model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.allow_custom_ops = True\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "os.chdir(HOME_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(MY_MODEL,\"inference_graph\",\"saved_model\"))\n",
    "converter.allow_custom_ops = True\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "# converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "converter.optimizations = [ tf.lite.Optimize.OPTIMIZE_FOR_SIZE ]\n",
    "# converter.optimizations = [ tf.lite.Optimize.EXPERIMENTAL_SPARSITY ]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "os.chdir(HOME_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(MY_MODEL, \"inference_tflite_graph\",\"model.tflite\"), num_threads=6)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Print input shape and type\n",
    "print(interpreter.get_input_details()[0]['shape'], interpreter.get_input_details()[0]['dtype'])\n",
    "# Print output shape and type\n",
    "for i in range(4):\n",
    "    print(interpreter.get_output_details()[i]['shape'], interpreter.get_output_details()[i]['dtype'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from models.research.object_detection.utils import label_map_util\n",
    "from models.research.object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(os.path.join(DATA_PATH, \"annotations.pbtxt\"), use_display_name=True)\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "scores_index = interpreter.get_output_details()[0]['index']\n",
    "boxes_index = interpreter.get_output_details()[1]['index']\n",
    "detection_num_index = interpreter.get_output_details()[2]['index']\n",
    "classes_index = interpreter.get_output_details()[3]['index']\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "def run_inference_and_display(image_path):\n",
    "    image_np = Image.open(image_path)\n",
    "    image_np = np.asarray(image_np.convert('RGB').resize((640,640)))\n",
    "\n",
    "    image_norm = (image_np.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    input_tensor = np.array(np.expand_dims(image_norm,0), dtype=np.float32)\n",
    "\n",
    "    interpreter.set_tensor(input_index, input_tensor)\n",
    "\n",
    "    t = time.time()\n",
    "    interpreter.invoke()\n",
    "    t = time.time() - t\n",
    "\n",
    "    scores = np.squeeze(interpreter.get_tensor(scores_index))\n",
    "    boxes = np.squeeze(interpreter.get_tensor(boxes_index))\n",
    "    classes = np.squeeze(interpreter.get_tensor(classes_index)).astype(np.int64) + 1\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        min_score_thresh=.6,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=4)\n",
    "\n",
    "    display(Image.fromarray(image_np))\n",
    "    print(\"inference time:\", round(t*1000), \"ms\")\n",
    "    print(\"Scores:\", scores * 100)\n",
    "    print(\"Scores:\", scores.sum() )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EXAMPLE_IMAGES_DATA_PATH = os.path.join(HOME_PATH, \"example_images\")\n",
    "images = list(filter(lambda x: str(x).endswith(\".jpg\"),os.listdir(EXAMPLE_IMAGES_DATA_PATH)))\n",
    "for image in images:\n",
    "    run_inference_and_display(os.path.join(EXAMPLE_IMAGES_DATA_PATH, image))\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from models.research.object_detection.utils import ops as utils_ops\n",
    "from models.research.object_detection.utils import label_map_util\n",
    "from models.research.object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "utils_ops.tf = tf.compat.v1\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "model = tf.saved_model.load(os.path.join(MY_MODEL,\"inference_graph\",\"saved_model\"))\n",
    "category_index = label_map_util.create_category_index_from_labelmap(os.path.join(DATA_PATH, \"annotations.pbtxt\"), use_display_name=True)\n",
    "\n",
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    # need_detection_key = ['detection_classes','detection_boxes','detection_masks','detection_scores']\n",
    "    need_detection_key = ['detection_classes','detection_boxes','detection_scores']\n",
    "    output_dict = {key: output_dict[key][0, :num_detections].numpy()\n",
    "               for key in need_detection_key}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            tf.convert_to_tensor(output_dict['detection_masks']), output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    return output_dict\n",
    "\n",
    "def show_inference(model, image_path):\n",
    "    image_np = np.array(Image.open(image_path))\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=4)\n",
    "\n",
    "    display(Image.fromarray(image_np))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EXAMPLE_IMAGES_DATA_PATH = os.path.join(HOME_PATH, \"example_images\")\n",
    "images = list(filter(lambda x: str(x).endswith(\".jpg\"),os.listdir(EXAMPLE_IMAGES_DATA_PATH)))\n",
    "for image in images:\n",
    "   show_inference(model, os.path.join(EXAMPLE_IMAGES_DATA_PATH, image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}