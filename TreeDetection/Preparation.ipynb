{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praca inżynierska\n",
    "Marcin Bobiński\n",
    "nr albumu: 297225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_PATH = os.getcwd()\n",
    "RESEARCH_PATH = os.path.join(HOME_PATH, \"models/research\")\n",
    "DATASET_PATH = os.path.join(HOME_PATH, \"Dataset\")\n",
    "PRETRAINED_MODELS_PATH = os.path.join(HOME_PATH, \"pretrained_models\")\n",
    "DATA_PATH = os.path.join(HOME_PATH, \"data\")\n",
    "TF_RECORD_PATH = os.path.join(DATA_PATH, \"tf_record\")\n",
    "MY_MODEL = os.path.join(HOME_PATH, \"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie\n",
    "## TensorFlow Object Detection Api\n",
    "Pierwszym krokiem jest sklonowanie repezytorium\n",
    "```\n",
    "git clone https://github.com/tensorflow/models.git\n",
    "```\n",
    "</br>\n",
    "\n",
    "Następnie należy zainstalować przy pomocy menadżera pakietów pip lub w wirtualnym środowisku Docker. W przypadku wyboru pierwszej opcji zalecana jest instalacja na nowym środowisku.\n",
    "\n",
    "1. <strong>Pip </strong> </br>\n",
    "Najpierw należy zainstalować Protobufa np. https://github.com/protocolbuffers/protobuf/releases </br>\n",
    "Oraz dodać go do ścieżki.\n",
    "Następnie, przy jego pomocy z poziomu katalogu models/research wywołać:\n",
    "```\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "```\n",
    "Z tego samego poziomu skopiować plik instalacyjny:\n",
    "```\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "```\n",
    "A następnie dokonać instalacji:\n",
    "```\n",
    "python -m pip install --use-feature=2020-resolver .\n",
    "```\n",
    "\n",
    "2. <strong> Docker </strong></br>\n",
    "Przygotowanie środowiska:\n",
    "```\n",
    "docker build -f research/object_detection/dockerfiles/tf2/Dockerfile -t od .\n",
    "```\n",
    "Uruchomienie środowiska:\n",
    "```\n",
    "docker run -it od\n",
    "```\n",
    "\n",
    "Test przeprowadzonej instalacji można wykonać przy pomocy komendy:\n",
    "```\n",
    "python object_detection/builders/model_builder_tf2_test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instalacja PIP\n",
    "os.chdir(RESEARCH_PATH)\n",
    "os.system(\"protoc object_detection/protos/*.proto --python_out=.\")\n",
    "os.system(\"cp object_detection/packages/tf2/setup.py .\")\n",
    "os.system(\"python -m pip install --use-feature=2020-resolver .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instalacja Docker\n",
    "os.chdir(HOME_PATH)\n",
    "os.system(\"docker build -f research/object_detection/dockerfiles/tf2/Dockerfile -t od .\")\n",
    "os.system(\"docker run -it od\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja poprawności instalacji\n",
    "os.chdir(RESEARCH_PATH)\n",
    "result = os.system(\"python object_detection/builders/model_builder_tf2_test.py\")\n",
    "assert result==0, \"Object detection api wasn't installed properly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pobranie pretrenowanych modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Stworzenie folderu dla pretrenowanych modeli\n",
    "os.chdir(HOME_PATH)\n",
    "PRETRAINED_MODELS_PATH = os.path.join(HOME_PATH, \"pretrained_models\")\n",
    "if not os.path.exists(PRETRAINED_MODELS_PATH):\n",
    "    os.makedirs(PRETRAINED_MODELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mask Rcnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"mask_rcnn\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
    "model_name =  \"mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "MASK_RCNN_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SSD mobilenet 640 v1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"ssd_mobilenet_v1_640\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
    "model_name =  \"ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "_ = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SSD mobilenet 640 v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"ssd_mobilenet_v2_640\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
    "model_name =  \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "SSD640_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SSD mobilenet320 v2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"ssd_mobilenet_v2_320\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
    "model_name =  \"ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "SSD320_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SSD Resnet101 640"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"ssd640resnet\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
    "model_name =  \"ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "SSD640RESNET_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deep_mac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"deepmac\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20210329/'\n",
    "model_name =  \"deepmac_1024x1024_coco17\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "DEEPMAC_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Centernet hg104 512x512"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"centernet\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200713/'\n",
    "model_name =  \"centernet_hg104_512x512_coco17_tpu-8\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "CENTERNET_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CenterNet MobileNetV2 FPN 512x512"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"centernet_mobile\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20210210/'\n",
    "model_name =  \"centernet_mobilenetv2fpn_512x512_coco17_od\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "model_dir = str(model_dir).replace(model_name, \"centernet_mobilenetv2_fpn_od\")\n",
    "\n",
    "CENTERNET_MOBILE_PATH = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### EfficientDet D1 640x640"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "name = \"efficientdet_d1\"\n",
    "base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
    "model_name =  \"efficientdet_d1_coco17_tpu-32\"\n",
    "model_file = model_name + '.tar.gz'\n",
    "\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name,\n",
    "    origin=base_url + model_file,\n",
    "    untar=True\n",
    ")\n",
    "_ = shutil.copytree(model_dir, os.path.join(PRETRAINED_MODELS_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pobranie datasetu\n",
    "\n",
    "### LabelMe\n",
    "Zdjęcia drzew pobrane ze strony: http://labelme.csail.mit.edu/Release3.0/\n",
    "</br>\n",
    "Linki do pobrania datasetu:\n",
    "- train: http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/training.tar.gz\n",
    "- test: http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/test.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "# Stworzenie folderu dla pobranego datasetu\n",
    "os.chdir(HOME_PATH)\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.makedirs(DATASET_PATH)\n",
    "\n",
    "name = \"labelme\"\n",
    "\n",
    "train_url = \"http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/training.tar.gz\"\n",
    "train_dir = tf.keras.utils.get_file(\n",
    "    fname=\"train\",\n",
    "    origin=train_url,\n",
    "    untar=True\n",
    ")\n",
    "_ = shutil.copytree(train_dir, os.path.join(DATASET_PATH, name, \"train\"))\n",
    "\n",
    "test_url = \"http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/test.tar.gz\"\n",
    "test_dir = tf.keras.utils.get_file(\n",
    "    fname=\"test\",\n",
    "    origin=test_url,\n",
    "    untar=True\n",
    ")\n",
    "_ = shutil.copytree(test_dir, os.path.join(DATASET_PATH, name, \"test\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stworzenie folderu dla pobranego datasetu\n",
    "name = \"open-images\"\n",
    "os.chdir(HOME_PATH)\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.makedirs(DATASET_PATH)\n",
    "if not os.path.exists(os.path.join(DATASET_PATH, name)):\n",
    "    os.makedirs(os.path.join(DATASET_PATH, name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=[\"Tree\"],\n",
    "    split=\"train\",\n",
    "    dataset_dir=os.path.join(DATASET_PATH, name),\n",
    "    max_samples = int(16000),\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=[\"Tree\"],\n",
    "    split=\"validation\",\n",
    "    dataset_dir=os.path.join(DATASET_PATH, name),\n",
    "    max_samples = int(2000),\n",
    "    seed=2\n",
    ")\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=[\"Tree\"],\n",
    "    split=\"test\",\n",
    "    dataset_dir=os.path.join(DATASET_PATH, name),\n",
    "    max_samples = int(2000),\n",
    "    seed=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "print(fo.list_datasets())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "ds = fo.load_dataset(\"open-images-v6-train-validation-100\")\n",
    "fo.launch_app(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowywanie zdjęć\n",
    "### LabelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# copy data only from test and split it\n",
    "_ = shutil.copytree(os.path.join(DATASET_PATH,\"test\"), os.path.join(DATA_PATH, \"train\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import prepare_data_utils as pdu\n",
    "\n",
    "pdu.filterAndPrepareDataFromLabelMe(DATA_PATH, \"train\", [\"tree\"], [\"trees\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from utils import prepare_data_utils as pdu\n",
    "\n",
    "pdu.flatten(os.path.join(DATA_PATH, \"train\"))\n",
    "\n",
    "shutil.rmtree(os.path.join(DATA_PATH, \"train/Images\" ))\n",
    "shutil.rmtree(os.path.join(DATA_PATH, \"train/Annotations\" ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "images = os.listdir(os.path.join(DATA_PATH, \"train\"))\n",
    "images = list(filter(lambda image: str(image).endswith(\".jpg\"), images))\n",
    "random.seed(0)\n",
    "random.shuffle(images)\n",
    "images = images[0:len(images)//5]\n",
    "middle_index = len(images)//2\n",
    "val_images = images[:middle_index]\n",
    "test_images = images[middle_index:]\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_PATH, \"validation\")):\n",
    "    os.makedirs(os.path.join(DATA_PATH, \"validation\"))\n",
    "\n",
    "for image in val_images:\n",
    "    shutil.move(\n",
    "        os.path.join(DATA_PATH, f\"train\\\\{image}\").replace(\".jpg\", \".xml\"),\n",
    "        os.path.join(DATA_PATH, f\"validation\\\\{image}\").replace(\".jpg\", \".xml\")\n",
    "    )\n",
    "    shutil.move(os.path.join(DATA_PATH, f\"train\\\\{image}\"), os.path.join(DATA_PATH, f\"validation\\\\{image}\"))\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_PATH, \"test\")):\n",
    "    os.makedirs(os.path.join(DATA_PATH, \"test\"))\n",
    "\n",
    "for image in test_images:\n",
    "    shutil.move(\n",
    "        os.path.join(DATA_PATH, f\"train\\\\{image}\").replace(\".jpg\", \".xml\"),\n",
    "        os.path.join(DATA_PATH, f\"test\\\\{image}\").replace(\".jpg\", \".xml\")\n",
    "    )\n",
    "    shutil.move(os.path.join(DATA_PATH, f\"train\\\\{image}\"), os.path.join(DATA_PATH, f\"test\\\\{image}\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import oidv6_to_voc as otv\n",
    "\n",
    "OID_PATH = os.path.join(DATASET_PATH, \"open-images\")\n",
    "\n",
    "for split in [\"train\",\"validation\",\"test\"]:\n",
    "    otv.convert(\n",
    "        [os.path.join(OID_PATH, split, \"labels\", \"detections.csv\")],\n",
    "        os.path.join(OID_PATH, split, \"metadata\", \"classes.csv\"),\n",
    "        os.path.join(OID_PATH, split, \"data\"),\n",
    "        os.path.join(OID_PATH, split, \"data\")\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import prepare_data_utils as pdu\n",
    "\n",
    "for split in [\"train\",\"validation\", \"test\"]:\n",
    "    pdu.filterAndPrepareDataFromOI(\n",
    "        os.path.join(DATASET_PATH, \"open-images\", split, \"data\"),\n",
    "        [\"Tree\"],\n",
    "        \"tree\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Własne anotacje LabelMe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from utils import prepare_data_utils as pdu\n",
    "#\n",
    "# for name in [\"train\",\"validation\", \"test\"]:\n",
    "#     pdu.prepateMyDataSet(\n",
    "#         os.path.join(DATASET_PATH, \"myData\", name)\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uczenie modelu\n",
    "## Przygotowanie plików tf record"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(TF_RECORD_PATH):\n",
    "    os.makedirs(TF_RECORD_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label me"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.tree_tf_record import create_tf_record_from_xml\n",
    "\n",
    "create_tf_record_from_xml(os.path.join(DATA_PATH, \"train\"), os.path.join(TF_RECORD_PATH, \"train\"), 25, False)\n",
    "create_tf_record_from_xml(os.path.join(DATA_PATH, \"validation\"), os.path.join(TF_RECORD_PATH, \"validation\"), 10, False)\n",
    "create_tf_record_from_xml(os.path.join(DATA_PATH, \"test\"), os.path.join(TF_RECORD_PATH, \"test\"), 10, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.tree_tf_record import create_tf_record_from_xml\n",
    "\n",
    "create_tf_record_from_xml(os.path.join(DATASET_PATH, \"open-images\", \"train\", \"data\"), os.path.join(TF_RECORD_PATH, \"train\"), 25, False)\n",
    "create_tf_record_from_xml(os.path.join(DATASET_PATH, \"open-images\", \"validation\", \"data\"), os.path.join(TF_RECORD_PATH, \"validation\"), 10, False)\n",
    "create_tf_record_from_xml(os.path.join(DATASET_PATH, \"open-images\", \"test\", \"data\"), os.path.join(TF_RECORD_PATH, \"test\"), 10, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wlasny dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.tree_tf_record import create_tf_record_from_xml\n",
    "\n",
    "create_tf_record_from_xml(os.path.join(DATASET_PATH, \"myDataset\", \"train\"), os.path.join(TF_RECORD_PATH, \"train\"), 25, False)\n",
    "create_tf_record_from_xml(os.path.join(DATASET_PATH, \"myDataset\", \"validation\"), os.path.join(TF_RECORD_PATH, \"validation\"), 10, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Label Map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, \"annotations.pbtxt\"), 'w') as file:\n",
    "    file.write(\"item { \\n\\tid: 1\\n\\tname: \\'tree\\'\\n}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir(HOME_PATH)\n",
    "if not os.path.exists(MY_MODEL):\n",
    "    os.makedirs(MY_MODEL)\n",
    "\n",
    "shutil.copy(\n",
    "    os.path.join(PRETRAINED_MODELS_PATH,\"ssd640\",\"pipeline.config\"),\n",
    "    MY_MODEL\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uczenie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training = f\"python object_detection/model_main_tf2.py \\\n",
    "--pipeline_config_path={os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "--model_dir={os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "--alsologtostderr\"\n",
    "print(training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensorboard = f\"tensorboard --logdir={os.path.join(MY_MODEL,'checkpoints')}\"\n",
    "print(tensorboard)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export the inference graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(RESEARCH_PATH)\n",
    "\n",
    "os.system(f\"python object_detection/exporter_main_v2.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path {os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "    --trained_checkpoint_dir {os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "    --output_directory {os.path.join(MY_MODEL, 'inference_graph')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export TF Lite graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(RESEARCH_PATH)\n",
    "\n",
    "os.system(f\"python object_detection/export_tflite_graph_tf2.py \\\n",
    "    --pipeline_config_path {os.path.join(MY_MODEL,'pipeline.config')} \\\n",
    "    --trained_checkpoint_dir {os.path.join(MY_MODEL,'checkpoints')} \\\n",
    "    --output_directory {os.path.join(MY_MODEL, 'inference_tflite_graph')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(MY_MODEL, 'inference_tflite_graph', 'saved_model')) # path to the SavedModel directory\n",
    "converter.allow_custom_ops = True\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mask-RCNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.saved_model.load(os.path.join(MY_MODEL,\"inference_graph\",\"saved_model\"))\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "keras_model = model.keras_model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.allow_custom_ops = True\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "os.chdir(os.path.join(MY_MODEL, 'inference_tflite_graph'))\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(MY_MODEL,\"inference_graph\",\"saved_model\"))\n",
    "converter.allow_custom_ops = True\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "# converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
    "converter.optimizations = [ tf.lite.Optimize.OPTIMIZE_FOR_SIZE ]\n",
    "# converter.optimizations = [ tf.lite.Optimize.EXPERIMENTAL_SPARSITY ]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.saved_model.load(os.path.join(MY_MODEL, \"inference_tflite_graph\",\"saved_model\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.signatures['serving_default'].output_shapes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(MY_MODEL, \"inference_tflite_graph\",\"model.tflite\"))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Print input shape and type\n",
    "print(interpreter.get_input_details()[0]['shape'])\n",
    "print(interpreter.get_input_details()[0]['dtype'])\n",
    "\n",
    "# Print output shape and type\n",
    "print(\"-\"*20)\n",
    "print(interpreter.get_output_details()[0]['shape'])\n",
    "print(interpreter.get_output_details()[0]['dtype'])\n",
    "print(\"-\"*20)\n",
    "print(interpreter.get_output_details()[1]['shape'])\n",
    "print(interpreter.get_output_details()[1]['dtype'])\n",
    "print(\"-\"*20)\n",
    "print(interpreter.get_output_details()[2]['shape'])\n",
    "print(interpreter.get_output_details()[2]['dtype'])\n",
    "print(\"-\"*20)\n",
    "print(interpreter.get_output_details()[3]['shape'])\n",
    "print(interpreter.get_output_details()[3]['dtype'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from models.research.object_detection.utils import ops as utils_ops\n",
    "from models.research.object_detection.utils import label_map_util\n",
    "from models.research.object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "utils_ops.tf = tf.compat.v1\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "model = tf.saved_model.load(os.path.join(MY_MODEL,\"inference_graph\",\"saved_model\"))\n",
    "category_index = label_map_util.create_category_index_from_labelmap(os.path.join(DATA_PATH, \"annotations.pbtxt\"), use_display_name=True)\n",
    "\n",
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    need_detection_key = ['detection_classes','detection_boxes','detection_masks','detection_scores']\n",
    "    output_dict = {key: output_dict[key][0, :num_detections].numpy()\n",
    "               for key in need_detection_key}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            tf.convert_to_tensor(output_dict['detection_masks']), output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    return output_dict\n",
    "\n",
    "def show_inference(model, image_path):\n",
    "    image_np = np.array(Image.open(image_path))\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=4)\n",
    "\n",
    "    display(Image.fromarray(image_np))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "images = list(filter(lambda x: str(x).endswith(\".jpg\"),os.listdir(DATA_PATH)))\n",
    "for image in images:\n",
    "    show_inference(model, os.path.join(DATA_PATH, image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}